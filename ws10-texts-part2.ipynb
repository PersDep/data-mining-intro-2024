{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbHlWheTvTYi"
      },
      "source": [
        "# Семинар 11: обработка текстовых данных\n",
        "\n",
        "## Вступление\n",
        "Многие практические задачи так или иначе могут вовлекать в себя работу с текстовыми данными, например:\n",
        "\n",
        "- классификация текстов\n",
        "    - анализ тональности (например, позитивный/негативный отзыв)\n",
        "    - фильтрация спама\n",
        "    - по теме или жанру\n",
        "- машинный перевод\n",
        "- распознавание и синтез речи\n",
        "- извлечение информации\n",
        "    - именованные сущности (например, извлечение имен, локаций, названий организаций)\n",
        "    - извлечение фактов и событий\n",
        "- кластеризация текстов\n",
        "- оптическое распознавание символов\n",
        "- проверка правописания\n",
        "- вопросно-ответные системы, информационный поиск\n",
        "- суммаризация текстов\n",
        "- генерация текстов\n",
        "\n",
        "В целом, алгоритм работы с текстовыми данными можно разбить на такие шаги:\n",
        "\n",
        "- предобработка сырых данных\n",
        "- токенизация (создание словаря)\n",
        "- обработка словаря (удаление стоп-слов и пунктуации)\n",
        "- обработка токенов (лемматизация / стемминг)\n",
        "- векторизация текста (bag of words, TF-IDF, etc)\n",
        "\n",
        "Сегодня мы познакомимся с основами работы с текстовыми данными: рассмотрим некоторые методы предобработки и простейшие алгоритмы векторизации.\n",
        "\n",
        "### План семинара\n",
        "1. Токенизация\n",
        "2. Стоп-слова и пунктуация\n",
        "3. Лемматизация и стемминг\n",
        "4. Bag-of-words и TD-IDF\n",
        "5. Решение задачи с текстовыми данными\n",
        "6. Регулярные выражения\n",
        "7. Немножко про категориальные признаки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5dNFUPHvTY6"
      },
      "source": [
        "## 4. Bag-of-words и TF-IDF\n",
        "\n",
        "Но как же все-таки работать с текстами, используя стандартные методы машинного обучения? Ведь нам нужны объекты выборки, которые описываются числами, а не словами. Иначе говоря, нам нужно *векторизовать* текстовые данные.\n",
        "\n",
        "### 4.1 Bag-of-words\n",
        "\n",
        "Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d.$ В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n",
        "$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n",
        "\n",
        "Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.095188Z",
          "start_time": "2023-04-18T16:50:58.040839Z"
        },
        "id": "x2eF8nZLvTY7"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"I like my cat.\",\n",
        "    \"My cat is the most perfect cat.\",\n",
        "    \"is this cat or is this bread?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.104626Z",
          "start_time": "2023-04-18T16:50:58.043952Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjG4gU5JvTY7",
        "outputId": "0d8650c5-7419-4689-ca67-c7729734991f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I like my cat',\n",
              " 'My cat is the most perfect cat',\n",
              " 'is this cat or is this bread']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts_tokenized = [\n",
        "    \" \".join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts\n",
        "]\n",
        "texts_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.104770Z",
          "start_time": "2023-04-18T16:50:58.046474Z"
        },
        "id": "NVomQglIvTY8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cnt_vec = CountVectorizer()\n",
        "X = cnt_vec.fit_transform(texts_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.104925Z",
          "start_time": "2023-04-18T16:50:58.049174Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwAKWt_4vTY9",
        "outputId": "1c8f8dad-89a9-4402-adae-916f76ae1631"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['like', 'my', 'cat', 'is', 'the', 'most', 'perfect', 'this', 'or', 'bread'])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnt_vec.vocabulary_.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.105067Z",
          "start_time": "2023-04-18T16:50:58.051691Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye63VKazvTY-",
        "outputId": "4dede0ed-ac68-4bad-ef3e-bb5be72f4dbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 14 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.105194Z",
          "start_time": "2023-04-18T16:50:58.054Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvYOzpALvTY-",
        "outputId": "0b3192e7-2546-4040-cac8-55b208b30071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
              "       [0, 2, 1, 0, 1, 1, 0, 1, 1, 0],\n",
              "       [1, 1, 2, 0, 0, 0, 1, 0, 0, 2]])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.105450Z",
          "start_time": "2023-04-18T16:50:58.056639Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "z40jdf5-vTY-",
        "outputId": "80b68939-a2b0-4ea1-909e-409e9b1b6310"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bread</th>\n",
              "      <th>cat</th>\n",
              "      <th>is</th>\n",
              "      <th>like</th>\n",
              "      <th>most</th>\n",
              "      <th>my</th>\n",
              "      <th>or</th>\n",
              "      <th>perfect</th>\n",
              "      <th>the</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bread  cat  is  like  most  my  or  perfect  the  this\n",
              "0      0    1   0     1     0   1   0        0    0     0\n",
              "1      0    2   1     0     1   1   0        1    1     0\n",
              "2      1    1   2     0     0   0   1        0    0     2"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X.toarray(), columns=cnt_vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tZsL30hvTY-"
      },
      "source": [
        "### 4.2 TF-IDF\n",
        "\n",
        "Заметим, что если слово часто встречается в одном тексте, но почти не встречается в других, то оно получает для данного текста большой вес, ровно так же, как и слова, которые часто встречаются в каждом тексте. Для того чтобы разделять эти такие слова, можно использовать статистическую меру TF-IDF, характеризующую важность слова для конкретного текста. Для каждого слова из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{C(t | d)}{\\sum\\limits_{k \\in d}C(k | d)},\n",
        "$$\n",
        "где $C(t | d)$ - число вхождений слова $t$ в текст $d$.\n",
        "\n",
        "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
        "$$\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d_i \\in D \\mid t \\in d_i\\}|}\\right)\n",
        "$$\n",
        "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов.\n",
        "\n",
        "В итоге каждому слову $t$ из текста $d$ теперь можно присвоить вес\n",
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "Интерпретировать формулу выше несложно: действительно, чем чаще данное слово встречается в данном тексте и чем реже в остальных, тем важнее оно для этого текста.\n",
        "\n",
        "Отметим, что в качестве TF и IDF можно использовать другие [определения](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.105509Z",
          "start_time": "2023-04-18T16:50:58.061637Z"
        },
        "id": "F0YcEWeqvTY_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X = tfidf_vec.fit_transform(texts_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.105598Z",
          "start_time": "2023-04-18T16:50:58.064598Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpCVQJG8vTY_",
        "outputId": "f8290c90-e00b-4cef-cff8-538702ff4ba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['like', 'my', 'cat', 'is', 'the', 'most', 'perfect', 'this', 'or', 'bread'])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_vec.vocabulary_.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.105710Z",
          "start_time": "2023-04-18T16:50:58.066937Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saJG0dWyvTZA",
        "outputId": "de21ac1d-4ae3-4cb3-86ed-d1a9dac5fe9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 14 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.105840Z",
          "start_time": "2023-04-18T16:50:58.073856Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t43byRVIvTZA",
        "outputId": "ad00d857-ded7-4f95-f58f-f41765ece04b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.42544054, 0.        , 0.72033345, 0.        ,\n",
              "        0.54783215, 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.50130994, 0.32276391, 0.        , 0.42439575,\n",
              "        0.32276391, 0.        , 0.42439575, 0.42439575, 0.        ],\n",
              "       [0.33976626, 0.20067143, 0.516802  , 0.        , 0.        ,\n",
              "        0.        , 0.33976626, 0.        , 0.        , 0.67953252]])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.106645Z",
          "start_time": "2023-04-18T16:50:58.078284Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "YGpSpl4KvTZA",
        "outputId": "266e8fdd-a1ae-4c1c-a4b8-2a950abd0e03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bread</th>\n",
              "      <th>cat</th>\n",
              "      <th>is</th>\n",
              "      <th>like</th>\n",
              "      <th>most</th>\n",
              "      <th>my</th>\n",
              "      <th>or</th>\n",
              "      <th>perfect</th>\n",
              "      <th>the</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425441</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.720333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.547832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.501310</td>\n",
              "      <td>0.322764</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424396</td>\n",
              "      <td>0.322764</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424396</td>\n",
              "      <td>0.424396</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.339766</td>\n",
              "      <td>0.200671</td>\n",
              "      <td>0.516802</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.339766</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.679533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      bread       cat        is      like      most        my        or   \n",
              "0  0.000000  0.425441  0.000000  0.720333  0.000000  0.547832  0.000000  \\\n",
              "1  0.000000  0.501310  0.322764  0.000000  0.424396  0.322764  0.000000   \n",
              "2  0.339766  0.200671  0.516802  0.000000  0.000000  0.000000  0.339766   \n",
              "\n",
              "    perfect       the      this  \n",
              "0  0.000000  0.000000  0.000000  \n",
              "1  0.424396  0.424396  0.000000  \n",
              "2  0.000000  0.000000  0.679533  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X.toarray(), columns=cnt_vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwcgx05UvTZA"
      },
      "source": [
        "**Вопросик:** что изменилось по сравнению с использованием метода `CountVectorizer`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrP93rWwvTZB"
      },
      "source": [
        "## 5. Решение задачи с текстовыми данными\n",
        "\n",
        "Будем решать задачу классификации твитов по тональности. Возьмём датасет из твитов, в котором про каждый твит известно, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску. Классификацию по тональности используют, например, в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
        "\n",
        "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.106702Z",
          "start_time": "2023-04-18T16:50:58.080790Z"
        },
        "id": "fpp8g-QkJY1r"
      },
      "outputs": [],
      "source": [
        "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
        "# !wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
        "# !wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.106741Z",
          "start_time": "2023-04-18T16:50:58.082275Z"
        },
        "id": "1oVlBKs_JY1r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MaxAbsScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.494315Z",
          "start_time": "2023-04-18T16:50:58.085576Z"
        },
        "id": "lD3sGsEeJY1r"
      },
      "outputs": [],
      "source": [
        "# считываем данные и заполняем общий датасет\n",
        "positive = pd.read_csv(\"positive.csv\", sep=\";\", usecols=[3], names=[\"text\"])\n",
        "positive[\"label\"] = \"positive\"\n",
        "negative = pd.read_csv(\"negative.csv\", sep=\";\", usecols=[3], names=[\"text\"])\n",
        "negative[\"label\"] = \"negative\"\n",
        "df = pd.concat([positive, negative])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.498928Z",
          "start_time": "2023-04-18T16:50:58.495906Z"
        },
        "id": "m5CxnRFFJY1r",
        "outputId": "8497cd0f-b60f-4bc5-ae3e-0a3fb741482a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111918</th>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111919</th>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111920</th>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111921</th>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111922</th>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text     label\n",
              "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
              "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
              "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
              "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
              "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.501134Z",
          "start_time": "2023-04-18T16:50:58.498598Z"
        },
        "id": "nrkAq7GoJY1r",
        "outputId": "ff48ad1f-7b9d-4e08-80e6-fc38dd054d4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(226834, 2)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.534375Z",
          "start_time": "2023-04-18T16:50:58.501394Z"
        },
        "id": "L3Abgj_vJY1r"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label, random_state=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnzNFeOWJY1s"
      },
      "source": [
        "#### n-граммы\n",
        "\n",
        "n-граммы — это последовательности n токенов из исходного текста. В простейшем случае это могут быть последовательности из букв или последовательности из слов. Давайте посмотрим подробнее на примере."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.535017Z",
          "start_time": "2023-04-18T16:50:58.516459Z"
        },
        "id": "mMoCnsi5vTZB"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.555122Z",
          "start_time": "2023-04-18T16:50:58.518609Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIf_da6RvTZD",
        "outputId": "9e811e97-bc9b-4bc4-a980-b164d1a5a575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = \"Если б мне платили каждый раз\".split()\n",
        "list(ngrams(sent, 1))  # униграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.555317Z",
          "start_time": "2023-04-18T16:50:58.521205Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhaYTVyuvTZD",
        "outputId": "dbd0ae5c-d5eb-4e1f-e501-7e54476014b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б'),\n",
              " ('б', 'мне'),\n",
              " ('мне', 'платили'),\n",
              " ('платили', 'каждый'),\n",
              " ('каждый', 'раз')]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 2))  # биграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.555416Z",
          "start_time": "2023-04-18T16:50:58.523529Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fpat_ZGvTZD",
        "outputId": "a49249bd-6199-41ff-85cd-fe108b2a684a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне'),\n",
              " ('б', 'мне', 'платили'),\n",
              " ('мне', 'платили', 'каждый'),\n",
              " ('платили', 'каждый', 'раз')]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 3))  # триграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:50:58.555533Z",
          "start_time": "2023-04-18T16:50:58.525914Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK2QdeycvTZE",
        "outputId": "c8dc6ac6-1ee8-4cc1-8526-47cd8dbfaa40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
              " ('б', 'мне', 'платили', 'каждый', 'раз')]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 5))  # ... пентаграммы?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo8QZI0wJY1s"
      },
      "source": [
        "В качестве альтернативы можно пользоваться `CountVectorizer`, который работает так:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности количества токенов в нашем словаре\n",
        "* заполняет каждый i-тый элемент количеством вхождений токена в данный документ\n",
        "\n",
        "Параметр `ngram_range` отвечает за то, какие n-граммы мы используем в качестве фичей:\n",
        "- `ngram_range=(1, 1)` — униграммы\n",
        "- `ngram_range=(3, 3)` — триграммы\n",
        "- `ngram_range=(1, 3)` — униграммы, биграммы и триграммы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXxk4fnuJY1s"
      },
      "source": [
        "### 5.1 Обучение моделей\n",
        "\n",
        "Давайте обучим наш первый бейзлайн — логрег на униграммах!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:51:05.699896Z",
          "start_time": "2023-04-18T16:50:58.589759Z"
        },
        "id": "oTJxQssTJY1s",
        "outputId": "04b53fa7-9990-460e-bb98-eea70434e281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.77      0.76     27957\n",
            "    positive       0.77      0.76      0.77     28752\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)  # bow — bag of words (мешок слов)\n",
        "bow_test = vec.transform(x_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a56F0vivTZL"
      },
      "source": [
        "Попробуем сделать то же самое для триграмм."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:51:32.529458Z",
          "start_time": "2023-04-18T16:51:05.755811Z"
        },
        "id": "5oQpnmvdJY1t",
        "outputId": "ece03877-e64a-4f51-e332-c35cb8ec0bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.47      0.57     27957\n",
            "    positive       0.61      0.82      0.70     28752\n",
            "\n",
            "    accuracy                           0.65     56709\n",
            "   macro avg       0.67      0.65      0.64     56709\n",
            "weighted avg       0.67      0.65      0.64     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(3, 3))\n",
        "bow = vec.fit_transform(x_train)\n",
        "bow_test = vec.transform(x_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred_thrgramm = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred_thrgramm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne9kOzRSJY1t"
      },
      "source": [
        "В нашем случае стало хуже :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyW9lA_6vTZL"
      },
      "source": [
        "А теперь повторим процедуру для TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:51:40.397191Z",
          "start_time": "2023-04-18T16:51:32.587689Z"
        },
        "id": "HMQXJdY9JY1t"
      },
      "outputs": [],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
        "vec_train = vec.fit_transform(x_train)\n",
        "vec_test = vec.transform(x_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "vec_train = scaler.fit_transform(vec_train)\n",
        "vec_test = scaler.transform(vec_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=300, random_state=42)\n",
        "clf.fit(vec_train, y_train)\n",
        "pred_tfidf = clf.predict(vec_test)\n",
        "print(classification_report(y_test, pred_tfidf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixapx51FvTZM"
      },
      "source": [
        "### 5.2 О важности эксплоративного анализа\n",
        "\n",
        "Но иногда пунктуация бывает и не шумом. Главное — отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:52:00.862268Z",
          "start_time": "2023-04-18T16:51:40.411317Z"
        },
        "id": "yQCYYor2JY1t"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
        "bow = vec.fit_transform(x_train)\n",
        "bow_test = vec.transform(x_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_8uz4TevTZM"
      },
      "source": [
        "Стоило оставить пунктуацию, и внезапно все метрики устремились к 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдём признак с самыми большим коэффициентом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LupaWa61JY1t"
      },
      "outputs": [],
      "source": [
        "vec.get_feature_names_out()[np.argmax(clf.coef_)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZC8Mb6bvTZM"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:52:01.743208Z",
          "start_time": "2023-04-18T16:52:00.882081Z"
        },
        "id": "vvWdQbYMvTZN"
      },
      "outputs": [],
      "source": [
        "cool_token = vec.get_feature_names_out()[np.argmax(clf.coef_)]\n",
        "pred = [\"positive\" if cool_token in tweet else \"negative\" for tweet in x_test]\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:52:01.799440Z",
          "start_time": "2023-04-18T16:52:01.743612Z"
        },
        "id": "3nD71B-ZvTZO"
      },
      "outputs": [],
      "source": [
        "cool_token = vec.get_feature_names_out()[np.argmax(clf.coef_)]\n",
        "tweets_with_cool_token = [tweet for tweet in x_train if cool_token in tweet]\n",
        "np.random.seed(42)\n",
        "for tweet in np.random.choice(tweets_with_cool_token, size=10, replace=False):\n",
        "    print(tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv6P8ncIvTZO"
      },
      "source": [
        "### 5.3 Символьные n-граммы\n",
        "\n",
        "Теперь в качестве признаков используем, например, униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-18T16:52:07.747526Z",
          "start_time": "2023-04-18T16:52:01.811832Z"
        },
        "id": "mO1dvXx3JY1u"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), analyzer=\"char\")\n",
        "bow = vec.fit_transform(x_train)\n",
        "bow_test = vec.transform(x_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PK8-_xwvTZO"
      },
      "source": [
        "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или иначе, на символах классифицировать тоже можно: для некоторых задач (например, для определения языка) признаки-символьные n-граммы могут внести серьезный вклад в качество модели. Ещё одна замечательная особенность признаков-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of sem10_texts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
